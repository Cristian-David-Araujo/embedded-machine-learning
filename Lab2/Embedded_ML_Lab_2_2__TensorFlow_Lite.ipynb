{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SFBFiQlYlva"
      },
      "source": [
        "# Embedded ML - Lab 2.2: TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svldvvGfmN8q"
      },
      "source": [
        "In this lab you will learn the basics of TensorFlow Lite, a complement of TensorFlow that allows you to optimize and run models on constrained devices. It provides a much lighter runtime than TensorFlow but it only supports a subset of the tools available in full TensorFlow.\n",
        "\n",
        "In this lab you might be given some helper functions but you are expected to write most of the code and be able to explain it at a high level of abstraction and also to modify any part of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQK0RRRuY3rJ"
      },
      "source": [
        "### Learning outcomes\n",
        "\n",
        "\n",
        "* Explain the basic concepts associated with TensorFlow Lite\n",
        "* Develop applications following the basic TensorFlow Lite workflow\n",
        "* Implement post-training quantization using TensorFlow Lite tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8wat6Kxul5R"
      },
      "source": [
        "### TensorFlow Lite workflow\n",
        "After having built a TensorFlow model, you can convert it to the TensorFlow Lite representation. Then you can run it with the TensorFlow Lite interpreter on your development environment before exporting it and copying it to the target device.\n",
        "\n",
        "To run the model with TensorFlow Lite you should load the model to the TensorFlow Lite interpreter, allocate the input/output tensors, pass the input data and finally run inference. Notice that TensorFlow Lite API calls are different from those of TensorFlow.\n",
        "\n",
        "In this part of the assignment, you should create and train a simple model (e.g. a one-neuron network) with TensorFlow and then save it. Then follow the TensorFlow Lite workflow until you are able to run inference and validate the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-22 23:10:37.529495: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-05-22 23:10:37.533022: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-05-22 23:10:37.542202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747955437.556869  132232 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747955437.561179  132232 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1747955437.573377  132232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747955437.573389  132232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747955437.573391  132232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747955437.573393  132232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-22 23:10:37.577810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#### TENSORFLOW BASIC WORKFLOW\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fs6U9xbNuz3u"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "2025-05-22 23:10:47.932697: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Predictions:\n",
            "Input: [0 0], Predicted: [0.46291617], Actual: [0]\n",
            "Input: [0 1], Predicted: [0.45899835], Actual: [1]\n",
            "Input: [1 0], Predicted: [0.648431], Actual: [1]\n",
            "Input: [1 1], Predicted: [0.46291617], Actual: [0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create the model for XOR\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(2, activation='relu', input_shape=(2,)),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Generate some data\n",
        "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "# Train the model\n",
        "y_train = np.array([[0], [1], [1], [0]])\n",
        "model.fit(x_train, y_train, epochs=500, verbose=0)  # Increased epochs for better training\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "x_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_test = np.array([[0], [1], [1], [0]])\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the predictions\n",
        "predictions = model.predict(x_test)\n",
        "print(\"Predictions:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Input: {x_test[i]}, Predicted: {pred}, Actual: {y_test[i]}\")\n",
        "\n",
        "\n",
        "# Save the model to a file\n",
        "model.save('xor_model.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OcQ6-6l8sHgz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpuhc1lkea/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpuhc1lkea/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/tmp/tmpuhc1lkea'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  126353262565264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353262568720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353262567184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353262572752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "W0000 00:00:1747955464.188171  132232 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1747955464.188196  132232 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-05-22 23:11:04.188513: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpuhc1lkea\n",
            "2025-05-22 23:11:04.188905: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-05-22 23:11:04.188912: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpuhc1lkea\n",
            "I0000 00:00:1747955464.191325  132232 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
            "2025-05-22 23:11:04.191761: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-05-22 23:11:04.205627: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpuhc1lkea\n",
            "2025-05-22 23:11:04.210325: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 21814 microseconds.\n",
            "2025-05-22 23:11:04.217596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
          ]
        }
      ],
      "source": [
        "#### TENSORFLOR LITE BASIC WORKFLOW\n",
        "\n",
        "# Load model of the XOR\n",
        "model = tf.keras.models.load_model('xor_model.keras')\n",
        "\n",
        "# Convert model to TF Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "# Save TF Lite model to a file\n",
        "import pathlib\n",
        "tflite_model_path = pathlib.Path('xor_model.tflite')\n",
        "write_file = tflite_model_path.write_bytes(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up input/output tensors\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ai-edge-litert in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.3.0)\n",
            "Requirement already satisfied: backports.strenum in /usr/local/python/3.12.1/lib/python3.12/site-packages (from ai-edge-litert) (1.2.8)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/python/3.12.1/lib/python3.12/site-packages (from ai-edge-litert) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from ai-edge-litert) (2.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from ai-edge-litert) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from ai-edge-litert) (4.12.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ai-edge-litert\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "interpreter = Interpreter(model_path=tflite_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l5bEfUbL89In"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF Lite Predictions:\n",
            "Input: [0. 0.], Predicted: [0.], Actual: 0\n",
            "Input: [0. 1.], Predicted: [0.], Actual: 1\n",
            "Input: [1. 0.], Predicted: [1.], Actual: 1\n",
            "Input: [1. 1.], Predicted: [0.], Actual: 0\n",
            "TF Lite Model Accuracy: 0.7500\n"
          ]
        }
      ],
      "source": [
        "#Allocate tensors on the interpreter\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Set input values and output memory \n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "test_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "\n",
        "print(\"TF Lite Predictions:\")\n",
        "def new_func(output_data):\n",
        "    return output_data[0]\n",
        "\n",
        "correct_predictions = 0\n",
        "\n",
        "for i, single_input in enumerate(test_input):\n",
        "    # Reshape input to match the expected shape (batch size of 1)\n",
        "    single_input = single_input.reshape(1, -1)\n",
        "    interpreter.set_tensor(input_details[0]['index'], single_input)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get outputs\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    print(f\"Input: {single_input[0]}, Predicted: {new_func(np.round(output_data))}, Actual: {y_test[i][0]}\")\n",
        "\n",
        "    # Check if the prediction is correct\n",
        "    if np.round(output_data) == y_test[i][0]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Accuracy\n",
        "accuracy = correct_predictions / len(test_input)\n",
        "print(f\"TF Lite Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpPgM38-6tFQ"
      },
      "source": [
        "### Vision model with TensorFlow Lite\n",
        "\n",
        "In this part of the assignment, you should import a small pre-trained model for a vision application that takes at most 1 MB. Then you should follow the TensorFlow Lite workflow until you are able to run inference and obtain the same results as with TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp47l03m9d/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp47l03m9d/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/tmp/tmp47l03m9d'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer_6')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  126353840504464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840817360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840818896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840819472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840817168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840820240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840819664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840821008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1747956527.122355  132232 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1747956527.122375  132232 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-05-22 23:28:47.122535: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp47l03m9d\n",
            "2025-05-22 23:28:47.122927: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-05-22 23:28:47.122934: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp47l03m9d\n",
            "2025-05-22 23:28:47.126515: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-05-22 23:28:47.149650: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp47l03m9d\n",
            "2025-05-22 23:28:47.156806: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 34273 microseconds.\n"
          ]
        }
      ],
      "source": [
        "# Load the model ccn_model.keras\n",
        "model_cnn = tf.keras.models.load_model('cnn_model.keras')\n",
        "# Convert model to TF Lite\n",
        "converter_cnn = tf.lite.TFLiteConverter.from_keras_model(model_cnn)\n",
        "tflite_model_cnn = converter_cnn.convert()\n",
        "\n",
        "# Save TF Lite model to a file\n",
        "tflite_model_cnn_path = pathlib.Path('ccn_model.tflite')\n",
        "write_file_cnn = tflite_model_cnn_path.write_bytes(tflite_model_cnn)\n",
        "\n",
        "# Set up input/output tensors\n",
        "interpreter_cnn = Interpreter(model_path=tflite_model_cnn_path)\n",
        "interpreter_cnn.allocate_tensors()\n",
        "# Set input values and output memory\n",
        "input_details_cnn = interpreter_cnn.get_input_details()\n",
        "output_details_cnn = interpreter_cnn.get_output_details()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load a test dataset MNIST Fashion (Only for testing)\n",
        "mnist = keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_test = x_test / 255.0\n",
        "x_test = x_test.astype(np.float32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "\n",
        "# Only use 100 samples for testing\n",
        "x_test = x_test[:100]\n",
        "y_test = y_test[:100]\n",
        "\n",
        "# Free memory of x_train and y_train\n",
        "del x_train\n",
        "del y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input details: [{'name': 'serving_default_input_layer_6:0', 'index': 0, 'shape': array([ 1, 28, 28,  1], dtype=int32), 'shape_signature': array([-1, 28, 28,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 17, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Input shape: [ 1 28 28  1]\n"
          ]
        }
      ],
      "source": [
        "#Show the format of input and output details of the model\n",
        "print(\"Input details:\", input_details_cnn)\n",
        "print(\"Output details:\", output_details_cnn)\n",
        "\n",
        "# Get the input shape\n",
        "input_shape_cnn = input_details_cnn[0]['shape']\n",
        "print(\"Input shape:\", input_shape_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF Lite CNN Predictions:\n",
            "Input: 0, Predicted: 9, Actual: 9\n",
            "Input: 1, Predicted: 2, Actual: 2\n",
            "Input: 2, Predicted: 1, Actual: 1\n",
            "Input: 3, Predicted: 1, Actual: 1\n",
            "Input: 4, Predicted: 6, Actual: 6\n",
            "Input: 5, Predicted: 1, Actual: 1\n",
            "Input: 6, Predicted: 4, Actual: 4\n",
            "Input: 7, Predicted: 4, Actual: 6\n",
            "Input: 8, Predicted: 5, Actual: 5\n",
            "Input: 9, Predicted: 7, Actual: 7\n",
            "Input: 10, Predicted: 4, Actual: 4\n",
            "Input: 11, Predicted: 5, Actual: 5\n",
            "Input: 12, Predicted: 7, Actual: 7\n",
            "Input: 13, Predicted: 3, Actual: 3\n",
            "Input: 14, Predicted: 4, Actual: 4\n",
            "Input: 15, Predicted: 1, Actual: 1\n",
            "Input: 16, Predicted: 2, Actual: 2\n",
            "Input: 17, Predicted: 4, Actual: 4\n",
            "Input: 18, Predicted: 8, Actual: 8\n",
            "Input: 19, Predicted: 0, Actual: 0\n",
            "Input: 20, Predicted: 2, Actual: 2\n",
            "Input: 21, Predicted: 7, Actual: 5\n",
            "Input: 22, Predicted: 7, Actual: 7\n",
            "Input: 23, Predicted: 5, Actual: 9\n",
            "Input: 24, Predicted: 1, Actual: 1\n",
            "Input: 25, Predicted: 4, Actual: 4\n",
            "Input: 26, Predicted: 4, Actual: 6\n",
            "Input: 27, Predicted: 0, Actual: 0\n",
            "Input: 28, Predicted: 9, Actual: 9\n",
            "Input: 29, Predicted: 4, Actual: 3\n",
            "Input: 30, Predicted: 8, Actual: 8\n",
            "Input: 31, Predicted: 8, Actual: 8\n",
            "Input: 32, Predicted: 4, Actual: 3\n",
            "Input: 33, Predicted: 3, Actual: 3\n",
            "Input: 34, Predicted: 2, Actual: 8\n",
            "Input: 35, Predicted: 0, Actual: 0\n",
            "Input: 36, Predicted: 7, Actual: 7\n",
            "Input: 37, Predicted: 5, Actual: 5\n",
            "Input: 38, Predicted: 7, Actual: 7\n",
            "Input: 39, Predicted: 9, Actual: 9\n",
            "Input: 40, Predicted: 6, Actual: 6\n",
            "Input: 41, Predicted: 1, Actual: 1\n",
            "Input: 42, Predicted: 0, Actual: 3\n",
            "Input: 43, Predicted: 9, Actual: 7\n",
            "Input: 44, Predicted: 4, Actual: 6\n",
            "Input: 45, Predicted: 7, Actual: 7\n",
            "Input: 46, Predicted: 2, Actual: 2\n",
            "Input: 47, Predicted: 1, Actual: 1\n",
            "Input: 48, Predicted: 4, Actual: 2\n",
            "Input: 49, Predicted: 6, Actual: 2\n",
            "Input: 50, Predicted: 4, Actual: 4\n",
            "Input: 51, Predicted: 4, Actual: 4\n",
            "Input: 52, Predicted: 5, Actual: 5\n",
            "Input: 53, Predicted: 8, Actual: 8\n",
            "Input: 54, Predicted: 2, Actual: 2\n",
            "Input: 55, Predicted: 6, Actual: 2\n",
            "Input: 56, Predicted: 8, Actual: 8\n",
            "Input: 57, Predicted: 6, Actual: 4\n",
            "Input: 58, Predicted: 8, Actual: 8\n",
            "Input: 59, Predicted: 0, Actual: 0\n",
            "Input: 60, Predicted: 7, Actual: 7\n",
            "Input: 61, Predicted: 7, Actual: 7\n",
            "Input: 62, Predicted: 4, Actual: 8\n",
            "Input: 63, Predicted: 5, Actual: 5\n",
            "Input: 64, Predicted: 1, Actual: 1\n",
            "Input: 65, Predicted: 1, Actual: 1\n",
            "Input: 66, Predicted: 4, Actual: 2\n",
            "Input: 67, Predicted: 4, Actual: 3\n",
            "Input: 68, Predicted: 7, Actual: 9\n",
            "Input: 69, Predicted: 8, Actual: 8\n",
            "Input: 70, Predicted: 7, Actual: 7\n",
            "Input: 71, Predicted: 0, Actual: 0\n",
            "Input: 72, Predicted: 2, Actual: 2\n",
            "Input: 73, Predicted: 0, Actual: 6\n",
            "Input: 74, Predicted: 4, Actual: 2\n",
            "Input: 75, Predicted: 4, Actual: 3\n",
            "Input: 76, Predicted: 1, Actual: 1\n",
            "Input: 77, Predicted: 2, Actual: 2\n",
            "Input: 78, Predicted: 8, Actual: 8\n",
            "Input: 79, Predicted: 4, Actual: 4\n",
            "Input: 80, Predicted: 1, Actual: 1\n",
            "Input: 81, Predicted: 8, Actual: 8\n",
            "Input: 82, Predicted: 5, Actual: 5\n",
            "Input: 83, Predicted: 9, Actual: 9\n",
            "Input: 84, Predicted: 5, Actual: 5\n",
            "Input: 85, Predicted: 0, Actual: 0\n",
            "Input: 86, Predicted: 3, Actual: 3\n",
            "Input: 87, Predicted: 2, Actual: 2\n",
            "Input: 88, Predicted: 0, Actual: 0\n",
            "Input: 89, Predicted: 6, Actual: 6\n",
            "Input: 90, Predicted: 5, Actual: 5\n",
            "Input: 91, Predicted: 3, Actual: 3\n",
            "Input: 92, Predicted: 6, Actual: 6\n",
            "Input: 93, Predicted: 7, Actual: 7\n",
            "Input: 94, Predicted: 1, Actual: 1\n",
            "Input: 95, Predicted: 8, Actual: 8\n",
            "Input: 96, Predicted: 0, Actual: 0\n",
            "Input: 97, Predicted: 1, Actual: 1\n",
            "Input: 98, Predicted: 4, Actual: 4\n",
            "Input: 99, Predicted: 2, Actual: 2\n",
            "TF Lite CNN Model Accuracy: 0.7900\n"
          ]
        }
      ],
      "source": [
        "correct_predictions_cnn = 0\n",
        "print(\"TF Lite CNN Predictions:\")\n",
        "\n",
        "def idex_func(output_data):\n",
        "    return np.argmax(output_data[0])\n",
        "\n",
        "for i, single_input in enumerate(x_test):\n",
        "    # Reshape input to match the expected shape (batch size of 1)\n",
        "    single_input = single_input.reshape(1, 28, 28, 1)\n",
        "    interpreter_cnn.set_tensor(input_details_cnn[0]['index'], single_input)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter_cnn.invoke()\n",
        "\n",
        "    # Get outputs\n",
        "    output_data = interpreter_cnn.get_tensor(output_details_cnn[0]['index'])\n",
        "    print(f\"Input: {i}, Predicted: {idex_func(output_data)}, Actual: {y_test[i]}\")\n",
        "\n",
        "    # Check if the prediction is correct\n",
        "    if idex_func(output_data) == y_test[i]:\n",
        "        correct_predictions_cnn += 1\n",
        "\n",
        "# Accuracy\n",
        "accuracy_cnn = correct_predictions_cnn / len(y_test)\n",
        "print(f\"TF Lite CNN Model Accuracy: {accuracy_cnn:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T25pOIS7kcym"
      },
      "source": [
        "### Post-training quantization\n",
        "Finally, in this part of the assignment you should activate quantization and convert the model again. Compare model size and accuracy of the compressed TensorFlow Lite model by using various configurations (investigate how) and against the uncompressed baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpx1_a9y8t/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpx1_a9y8t/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/tmp/tmpx1_a9y8t'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer_6')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  126353840504464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840817360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840818896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840819472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840817168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840820240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840819664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  126353840821008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n",
            "W0000 00:00:1747957520.918208  132232 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1747957520.918226  132232 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-05-22 23:45:20.918371: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpx1_a9y8t\n",
            "2025-05-22 23:45:20.918841: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-05-22 23:45:20.918848: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpx1_a9y8t\n",
            "2025-05-22 23:45:20.923942: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-05-22 23:45:20.949448: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpx1_a9y8t\n",
            "2025-05-22 23:45:20.956622: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 38254 microseconds.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "You need to provide either a dictionary with input names and values, a tuple with signature key and a dictionary with input names and values, or an array with input values in the order of input tensors of the graph in the representative_dataset function. Unsupported value from dataset: [[[0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.01176471 0.00392157 0.         0.         0.02745098\n   0.         0.14509805 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.00392157 0.00784314 0.         0.10588235 0.32941177\n   0.04313726 0.         0.         0.         0.         0.\n   0.         0.46666667 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.00392157 0.         0.         0.34509805 0.56078434\n   0.43137255 0.         0.         0.         0.         0.08627451\n   0.3647059  0.41568628 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.01568628 0.         0.20784314 0.5058824  0.47058824\n   0.5764706  0.6862745  0.6156863  0.6509804  0.5294118  0.6039216\n   0.65882355 0.54901963 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.00784314 0.         0.04313726 0.5372549  0.50980395 0.5019608\n   0.627451   0.6901961  0.62352943 0.654902   0.69803923 0.58431375\n   0.5921569  0.5647059  0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.00392157 0.         0.00784314 0.00392157 0.         0.01176471\n   0.         0.         0.4509804  0.44705883 0.41568628 0.5372549\n   0.65882355 0.6        0.6117647  0.64705884 0.654902   0.56078434\n   0.6156863  0.61960787 0.04313726 0.        ]\n  [0.         0.         0.         0.         0.00392157 0.\n   0.         0.         0.         0.         0.01176471 0.\n   0.         0.34901962 0.54509807 0.3529412  0.36862746 0.6\n   0.58431375 0.5137255  0.5921569  0.6627451  0.6745098  0.56078434\n   0.62352943 0.6627451  0.1882353  0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.00784314 0.01568628 0.00392157 0.         0.         0.\n   0.38431373 0.53333336 0.43137255 0.42745098 0.43137255 0.63529414\n   0.5294118  0.5647059  0.58431375 0.62352943 0.654902   0.5647059\n   0.61960787 0.6627451  0.46666667 0.        ]\n  [0.         0.         0.00784314 0.00784314 0.00392157 0.00784314\n   0.         0.         0.         0.         0.10196079 0.42352942\n   0.45882353 0.3882353  0.43529412 0.45882353 0.53333336 0.6117647\n   0.5254902  0.6039216  0.6039216  0.6117647  0.627451   0.5529412\n   0.5764706  0.6117647  0.69803923 0.        ]\n  [0.01176471 0.         0.         0.         0.         0.\n   0.         0.08235294 0.20784314 0.36078432 0.45882353 0.43529412\n   0.40392157 0.4509804  0.5058824  0.5254902  0.56078434 0.6039216\n   0.64705884 0.6666667  0.6039216  0.5921569  0.6039216  0.56078434\n   0.5411765  0.5882353  0.64705884 0.16862746]\n  [0.         0.         0.09019608 0.21176471 0.25490198 0.29803923\n   0.33333334 0.4627451  0.5019608  0.48235294 0.43529412 0.44313726\n   0.4627451  0.49803922 0.49019608 0.54509807 0.52156866 0.53333336\n   0.627451   0.54901963 0.60784316 0.6313726  0.5647059  0.60784316\n   0.6745098  0.6313726  0.7411765  0.24313726]\n  [0.         0.26666668 0.36862746 0.3529412  0.43529412 0.44705883\n   0.43529412 0.44705883 0.4509804  0.49803922 0.5294118  0.53333336\n   0.56078434 0.49411765 0.49803922 0.5921569  0.6039216  0.56078434\n   0.5803922  0.49019608 0.63529414 0.63529414 0.5647059  0.5411765\n   0.6        0.63529414 0.76862746 0.22745098]\n  [0.27450982 0.6627451  0.5058824  0.40784314 0.38431373 0.39215687\n   0.36862746 0.38039216 0.38431373 0.4        0.42352942 0.41568628\n   0.46666667 0.47058824 0.5058824  0.58431375 0.6117647  0.654902\n   0.74509805 0.74509805 0.76862746 0.7764706  0.7764706  0.73333335\n   0.77254903 0.7411765  0.72156864 0.14117648]\n  [0.0627451  0.49411765 0.67058825 0.7372549  0.7372549  0.72156864\n   0.67058825 0.6        0.5294118  0.47058824 0.49411765 0.49803922\n   0.57254905 0.7254902  0.7647059  0.81960785 0.8156863  1.\n   0.81960785 0.69411767 0.9607843  0.9882353  0.9843137  0.9843137\n   0.96862745 0.8627451  0.80784315 0.19215687]\n  [0.         0.         0.         0.04705882 0.2627451  0.41568628\n   0.6431373  0.7254902  0.78039217 0.8235294  0.827451   0.8235294\n   0.8156863  0.74509805 0.5882353  0.32156864 0.03137255 0.\n   0.         0.         0.69803923 0.8156863  0.7372549  0.6862745\n   0.63529414 0.61960787 0.5921569  0.04313726]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]]].",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m converter_cnn.representative_dataset = \u001b[38;5;28;01mlambda\u001b[39;00m: (tf.data.Dataset.from_tensor_slices(x_test.astype(np.float32))\n\u001b[32m      4\u001b[39m                                                   .batch(\u001b[32m1\u001b[39m))\n\u001b[32m      5\u001b[39m converter_cnn.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m quantized_tflite_model_cnn = \u001b[43mconverter_cnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Save the quantized model to a file\u001b[39;00m\n\u001b[32m      9\u001b[39m quantized_tflite_model_cnn_path = pathlib.Path(\u001b[33m'\u001b[39m\u001b[33mquantized_cnn_model.tflite\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1250\u001b[39m, in \u001b[36m_export_metrics.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1247\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(convert_func)\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1249\u001b[39m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1202\u001b[39m, in \u001b[36mTFLiteConverterBase._convert_and_export_metrics\u001b[39m\u001b[34m(self, convert_func, *args, **kwargs)\u001b[39m\n\u001b[32m   1200\u001b[39m \u001b[38;5;28mself\u001b[39m._save_conversion_params_metric()\n\u001b[32m   1201\u001b[39m start_time = time.process_time()\n\u001b[32m-> \u001b[39m\u001b[32m1202\u001b[39m result = \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1203\u001b[39m elapsed_time_ms = (time.process_time() - start_time) * \u001b[32m1000\u001b[39m\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1768\u001b[39m, in \u001b[36mTFLiteKerasModelConverterV2.convert\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1755\u001b[39m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1757\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[32m   1758\u001b[39m \n\u001b[32m   1759\u001b[39m \u001b[33;03m  Returns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1766\u001b[39m \u001b[33;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1768\u001b[39m   saved_model_convert_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1769\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[32m   1770\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1749\u001b[39m, in \u001b[36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1745\u001b[39m   graph_def, input_tensors, output_tensors = (\n\u001b[32m   1746\u001b[39m       \u001b[38;5;28mself\u001b[39m._convert_keras_to_saved_model(temp_dir)\n\u001b[32m   1747\u001b[39m   )\n\u001b[32m   1748\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.saved_model_dir:\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1753\u001b[39m   shutil.rmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1494\u001b[39m, in \u001b[36mTFLiteConverterBaseV2.convert\u001b[39m\u001b[34m(self, graph_def, input_tensors, output_tensors)\u001b[39m\n\u001b[32m   1486\u001b[39m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[32m   1487\u001b[39m result = _convert_graphdef(\n\u001b[32m   1488\u001b[39m     input_data=graph_def,\n\u001b[32m   1489\u001b[39m     input_tensors=input_tensors,\n\u001b[32m   1490\u001b[39m     output_tensors=output_tensors,\n\u001b[32m   1491\u001b[39m     **converter_kwargs,\n\u001b[32m   1492\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimize_tflite_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_quant_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_build_conversion_flags\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconverter_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_io\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperimental_new_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    214\u001b[39m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    204\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m converter_error.errors:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1146\u001b[39m, in \u001b[36mTFLiteConverterBase._optimize_tflite_model\u001b[39m\u001b[34m(self, model, quant_mode, debug_options, quant_io)\u001b[39m\n\u001b[32m   1144\u001b[39m   q_allow_float = quant_mode.is_allow_float()\n\u001b[32m   1145\u001b[39m   q_variable_quantization = quant_mode.enable_mlir_variable_quantization\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m   model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m      \u001b[49m\u001b[43mq_in_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m      \u001b[49m\u001b[43mq_out_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m      \u001b[49m\u001b[43mq_activations_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m      \u001b[49m\u001b[43mq_bias_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m      \u001b[49m\u001b[43mq_allow_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m      \u001b[49m\u001b[43mq_variable_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdebug_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1157\u001b[39m m_in_type = in_type \u001b[38;5;28;01mif\u001b[39;00m in_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes.float32\n\u001b[32m   1158\u001b[39m m_out_type = out_type \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes.float32\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:760\u001b[39m, in \u001b[36mTFLiteConverterBase._quantize\u001b[39m\u001b[34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization, debug_options)\u001b[39m\n\u001b[32m    756\u001b[39m calibrate_quantize = _calibrator.Calibrator(\n\u001b[32m    757\u001b[39m     result, custom_op_registerers_by_name, custom_op_registerers_by_func\n\u001b[32m    758\u001b[39m )\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._experimental_calibrate_only \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.experimental_new_quantizer:\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m   calibrated = \u001b[43mcalibrate_quantize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalibrate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrepresentative_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_gen\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._experimental_calibrate_only:\n\u001b[32m    765\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m calibrated\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    214\u001b[39m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    204\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m converter_error.errors:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/optimize/calibrator.py:258\u001b[39m, in \u001b[36mCalibrator.calibrate\u001b[39m\u001b[34m(self, dataset_gen)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;129m@convert_phase\u001b[39m(Component.OPTIMIZE_TFLITE_MODEL, SubComponent.CALIBRATE)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalibrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_gen):\n\u001b[32m    250\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[32m    251\u001b[39m \n\u001b[32m    252\u001b[39m \u001b[33;03m  Returns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m \u001b[33;03m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feed_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._calibrator.Calibrate()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tensorflow/lite/python/optimize/calibrator.py:126\u001b[39m, in \u001b[36mCalibrator._feed_tensors\u001b[39m\u001b[34m(self, dataset_gen, resize_input)\u001b[39m\n\u001b[32m    124\u001b[39m   input_array = sample\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    127\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mYou need to provide either a dictionary with input \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mnames and values, a tuple with signature key and a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mdictionary with input names and values, or an array \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mwith input values in the order of input tensors of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mthe graph in the representative_dataset function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mUnsupported value from dataset: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(sample)\n\u001b[32m    133\u001b[39m   )\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m signature_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m initialized:\n\u001b[32m    136\u001b[39m   initialized[signature_key] = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: You need to provide either a dictionary with input names and values, a tuple with signature key and a dictionary with input names and values, or an array with input values in the order of input tensors of the graph in the representative_dataset function. Unsupported value from dataset: [[[0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.01176471 0.00392157 0.         0.         0.02745098\n   0.         0.14509805 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.00392157 0.00784314 0.         0.10588235 0.32941177\n   0.04313726 0.         0.         0.         0.         0.\n   0.         0.46666667 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.00392157 0.         0.         0.34509805 0.56078434\n   0.43137255 0.         0.         0.         0.         0.08627451\n   0.3647059  0.41568628 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.01568628 0.         0.20784314 0.5058824  0.47058824\n   0.5764706  0.6862745  0.6156863  0.6509804  0.5294118  0.6039216\n   0.65882355 0.54901963 0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.00784314 0.         0.04313726 0.5372549  0.50980395 0.5019608\n   0.627451   0.6901961  0.62352943 0.654902   0.69803923 0.58431375\n   0.5921569  0.5647059  0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.00392157 0.         0.00784314 0.00392157 0.         0.01176471\n   0.         0.         0.4509804  0.44705883 0.41568628 0.5372549\n   0.65882355 0.6        0.6117647  0.64705884 0.654902   0.56078434\n   0.6156863  0.61960787 0.04313726 0.        ]\n  [0.         0.         0.         0.         0.00392157 0.\n   0.         0.         0.         0.         0.01176471 0.\n   0.         0.34901962 0.54509807 0.3529412  0.36862746 0.6\n   0.58431375 0.5137255  0.5921569  0.6627451  0.6745098  0.56078434\n   0.62352943 0.6627451  0.1882353  0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.00784314 0.01568628 0.00392157 0.         0.         0.\n   0.38431373 0.53333336 0.43137255 0.42745098 0.43137255 0.63529414\n   0.5294118  0.5647059  0.58431375 0.62352943 0.654902   0.5647059\n   0.61960787 0.6627451  0.46666667 0.        ]\n  [0.         0.         0.00784314 0.00784314 0.00392157 0.00784314\n   0.         0.         0.         0.         0.10196079 0.42352942\n   0.45882353 0.3882353  0.43529412 0.45882353 0.53333336 0.6117647\n   0.5254902  0.6039216  0.6039216  0.6117647  0.627451   0.5529412\n   0.5764706  0.6117647  0.69803923 0.        ]\n  [0.01176471 0.         0.         0.         0.         0.\n   0.         0.08235294 0.20784314 0.36078432 0.45882353 0.43529412\n   0.40392157 0.4509804  0.5058824  0.5254902  0.56078434 0.6039216\n   0.64705884 0.6666667  0.6039216  0.5921569  0.6039216  0.56078434\n   0.5411765  0.5882353  0.64705884 0.16862746]\n  [0.         0.         0.09019608 0.21176471 0.25490198 0.29803923\n   0.33333334 0.4627451  0.5019608  0.48235294 0.43529412 0.44313726\n   0.4627451  0.49803922 0.49019608 0.54509807 0.52156866 0.53333336\n   0.627451   0.54901963 0.60784316 0.6313726  0.5647059  0.60784316\n   0.6745098  0.6313726  0.7411765  0.24313726]\n  [0.         0.26666668 0.36862746 0.3529412  0.43529412 0.44705883\n   0.43529412 0.44705883 0.4509804  0.49803922 0.5294118  0.53333336\n   0.56078434 0.49411765 0.49803922 0.5921569  0.6039216  0.56078434\n   0.5803922  0.49019608 0.63529414 0.63529414 0.5647059  0.5411765\n   0.6        0.63529414 0.76862746 0.22745098]\n  [0.27450982 0.6627451  0.5058824  0.40784314 0.38431373 0.39215687\n   0.36862746 0.38039216 0.38431373 0.4        0.42352942 0.41568628\n   0.46666667 0.47058824 0.5058824  0.58431375 0.6117647  0.654902\n   0.74509805 0.74509805 0.76862746 0.7764706  0.7764706  0.73333335\n   0.77254903 0.7411765  0.72156864 0.14117648]\n  [0.0627451  0.49411765 0.67058825 0.7372549  0.7372549  0.72156864\n   0.67058825 0.6        0.5294118  0.47058824 0.49411765 0.49803922\n   0.57254905 0.7254902  0.7647059  0.81960785 0.8156863  1.\n   0.81960785 0.69411767 0.9607843  0.9882353  0.9843137  0.9843137\n   0.96862745 0.8627451  0.80784315 0.19215687]\n  [0.         0.         0.         0.04705882 0.2627451  0.41568628\n   0.6431373  0.7254902  0.78039217 0.8235294  0.827451   0.8235294\n   0.8156863  0.74509805 0.5882353  0.32156864 0.03137255 0.\n   0.         0.         0.69803923 0.8156863  0.7372549  0.6862745\n   0.63529414 0.61960787 0.5921569  0.04313726]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.         0.         0.\n   0.         0.         0.         0.        ]]]."
          ]
        }
      ],
      "source": [
        "# Apply post-training quantization with INT8\n",
        "converter_cnn.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter_cnn.representative_dataset = lambda: (tf.data.Dataset.from_tensor_slices(x_test.astype(np.float32))\n",
        "                                                  .batch(1))\n",
        "converter_cnn.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "quantized_tflite_model_cnn = converter_cnn.convert()\n",
        "\n",
        "# Save the quantized model to a file\n",
        "quantized_tflite_model_cnn_path = pathlib.Path('quantized_cnn_model.tflite')\n",
        "quantized_write_file_cnn = quantized_tflite_model_cnn_path.write_bytes(quantized_tflite_model_cnn)\n",
        "\n",
        "# Compare model sizes\n",
        "print(f\"Uncompressed model size: {write_file_cnn} bytes\")\n",
        "print(f\"Quantized model size: {quantized_write_file_cnn} bytes\")\n",
        "\n",
        "# Load the quantized model\n",
        "quantized_interpreter_cnn = Interpreter(model_path=quantized_tflite_model_cnn_path)\n",
        "quantized_interpreter_cnn.allocate_tensors()\n",
        "\n",
        "# Set input/output tensors for the quantized model\n",
        "quantized_input_details_cnn = quantized_interpreter_cnn.get_input_details()\n",
        "quantized_output_details_cnn = quantized_interpreter_cnn.get_output_details()\n",
        "\n",
        "# Evaluate the quantized model's accuracy\n",
        "correct_predictions_quantized_cnn = 0\n",
        "print(\"Quantized TF Lite CNN Predictions:\")\n",
        "\n",
        "for i, single_input in enumerate(x_test):\n",
        "    # Reshape input to match the expected shape (batch size of 1)\n",
        "    single_input = single_input.reshape(1, 28, 28, 1)\n",
        "    # Ensure input is quantized to match the model's expected input type\n",
        "    input_scale, input_zero_point = quantized_input_details_cnn[0]['quantization']\n",
        "    single_input_quantized = (single_input / input_scale + input_zero_point).astype(quantized_input_details_cnn[0]['dtype'])\n",
        "    quantized_interpreter_cnn.set_tensor(quantized_input_details_cnn[0]['index'], single_input_quantized)\n",
        "\n",
        "    # Run inference\n",
        "    quantized_interpreter_cnn.invoke()\n",
        "\n",
        "    # Get outputs\n",
        "    output_data = quantized_interpreter_cnn.get_tensor(quantized_output_details_cnn[0]['index'])\n",
        "    print(f\"Input: {i}, Predicted: {idex_func(output_data)}, Actual: {y_test[i]}\")\n",
        "\n",
        "    # Check if the prediction is correct\n",
        "    if idex_func(output_data) == y_test[i]:\n",
        "        correct_predictions_quantized_cnn += 1\n",
        "\n",
        "# Quantized model accuracy\n",
        "accuracy_quantized_cnn = correct_predictions_quantized_cnn / len(y_test)\n",
        "print(f\"Quantized TF Lite CNN Model Accuracy: {accuracy_quantized_cnn:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
