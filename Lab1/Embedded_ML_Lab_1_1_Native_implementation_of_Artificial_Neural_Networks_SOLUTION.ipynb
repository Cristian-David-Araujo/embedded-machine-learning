{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Embedded ML - Lab 1.1: Native implementation of Artificial Neural Netwroks"
      ],
      "metadata": {
        "id": "7SFBFiQlYlva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab you are asked to write the code for an Artificial Neural Network (ANN) without using ML libraries such as SciKit-learn, PyTorch or TensorFlow, but you are allowed to use standard libraries such as math, numpy and matplotlib if needed. You are given some code but you are expected to write some more and be able to explain and modify everything. This is a key foundational exercise for you to understand the efficiency aspects that will be dealt with throughout this course."
      ],
      "metadata": {
        "id": "svldvvGfmN8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning outcomes\n",
        "\n",
        "\n",
        "* Explain the basic concepts of ANNs\n",
        "* Implement simple ANNs in Python without using advanced libraries\n",
        "* Analyze the computational resources demanded when training, running inference and scaling ANNs\n",
        "\n"
      ],
      "metadata": {
        "id": "lQK0RRRuY3rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Linear regression\n",
        "Linear regression is perhaps the simplest form of ML and can be thought of as an ANN with a single neuron. Yet, it can make a linear approximation of an input-output pair of data arrays.\n",
        "\n",
        "Below is an incomplete code for a Python class that implements a linear regressor. You should **complete the missing code** for the predict() and error() methods and then write a simple implementation of the class."
      ],
      "metadata": {
        "id": "l8wat6Kxul5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT1_9yYzX6Wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3353b8-e05f-4348-d251-d116d34c76ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real Y is [-3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
            "My Y is   [-3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
            "My loss is: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Solution to Linear Regression excersize\n",
        "\n",
        "# Ref 1: https://github.com/tinyMLx/colabs/blob/master/2-1-4-ExploringLoss.ipynb\n",
        "# Ref 2: https://github.com/tinyMLx/colabs/blob/master/2-1-6-MimimizingLoss.ipynb\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "class LinRegressor:\n",
        "  def __init__(self, w, b):\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "\n",
        "  def predict(self, x):\n",
        "    self.myY = []\n",
        "\n",
        "    for thisX in x:\n",
        "      thisY = self.w * thisX + self.b\n",
        "      self.myY.append(thisY)\n",
        "\n",
        "    return self.myY\n",
        "\n",
        "  def error(self, y):\n",
        "    total_square_error = 0\n",
        "    for i in range(0, len(y)):\n",
        "      square_error = (y[i] - self.myY[i]) ** 2\n",
        "      total_square_error += square_error\n",
        "\n",
        "    return math.sqrt(total_square_error)\n",
        "\n",
        "dataset_A = {\n",
        "    # y = 2x - 1\n",
        "    \"x\": [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    \"y\": [-3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
        "    }\n",
        "\n",
        "dataset_B = {\n",
        "    # y = ((x/5) + 1)^2 - 3\n",
        "    \"x\": [-8, -5, -3.4, -2, 0, 1.9, 4, 6.2, 8, 11.5],\n",
        "    \"y\": [-2.64, -3, -2.9, -2.64, -2, -1.09, 0.24, 2.01, 3.76, 7.89]\n",
        "    }\n",
        "\n",
        "w = 2 #random.randint(-10, 10)\n",
        "b = -1 #random.randint(-10, 10)\n",
        "\n",
        "x = dataset_A[\"x\"]\n",
        "y = dataset_A[\"y\"]\n",
        "\n",
        "model = LinRegressor(w, b)\n",
        "\n",
        "print(\"Real Y is \" + str(y))\n",
        "print(\"My Y is   \" + str(model.predict(x)))\n",
        "print(\"My loss is: \" + str(model.error(y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measure the error for three different sets of parameter values, for each dataset. **Plot the datasets against the predictions** and analyze the model results obtained.\n",
        "\n",
        "*   Can the error of dataset A be zero?\n",
        "*   Can the error of dataset B be zero?\n",
        "*   A zero error means that the model represents the system perfectly?\n",
        "*   Can you model any kind of system with this type of model?"
      ],
      "metadata": {
        "id": "4H_xvt35YLGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Artificial Neural Networks\n",
        "Based on the principles of aproximating the mathematical relationship between two arrays of data, ANNs are scaled up algorithms that connect multiple linear regressors with activation functions in order to detect more complex relationships between data. The computation elements that make up an ANN are called Perceptrons or simply neurons, and they are topologically organized in layers.\n",
        "\n",
        "Given is a Python code that partially implements a neural network with three layers: input, hidden and output. It defines methods for training and inference and uses the XOR function as a test case.\n",
        "\n",
        "Study the code to get familiar with it and **complete the implementation of the forward()** method that takes in the network inputs to produce the outputs.\n",
        "Verify the network works by running the code and observing the error going down and producing corrects results. Also play with the training parameters to see how learning improves or degrades.\n",
        "\n",
        "Then mode to **implementing the my_dot() method to replace NumPy's dot()**, in order to make explicit the operations that are executed every time the method is called. Modify the forward method to use the new function and verify its correctness."
      ],
      "metadata": {
        "id": "YpQD3Y7a_wFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.bias_input_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.bias_hidden_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def my_dot(self, A, B):\n",
        "        \"\"\"\n",
        "        Custom implementation of the dot product between two matrices or vectors.\n",
        "        \"\"\"\n",
        "        A = np.atleast_2d(A)  # Ensure A is at least 2D\n",
        "        B = np.atleast_2d(B)  # Ensure B is at least 2D\n",
        "\n",
        "        rows_A, cols_A = A.shape\n",
        "        rows_B, cols_B = B.shape\n",
        "\n",
        "        # Ensure matrices can be multiplied\n",
        "        if cols_A != rows_B:\n",
        "            raise ValueError(\"Incompatible matrix dimensions for multiplication\")\n",
        "\n",
        "        # Initialize result matrix with zeros\n",
        "        result = np.zeros((rows_A, cols_B))\n",
        "\n",
        "        # Perform matrix multiplication\n",
        "        for i in range(rows_A):\n",
        "            for j in range(cols_B):\n",
        "                for k in range(cols_A):\n",
        "                    result[i][j] += A[i, k] * B[k, j]\n",
        "\n",
        "        return result if result.shape[0] > 1 else result.flatten()  # Return a flattened array if it's a single row\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward propagation through the network\n",
        "        self.hidden_output = self.sigmoid(self.my_dot(x, self.weights_input_hidden) + self.bias_input_hidden)\n",
        "        self.output = self.sigmoid(self.my_dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, x, y, output, learning_rate):\n",
        "        # Backpropagation and weight updates\n",
        "        self.error = y - output\n",
        "        d_output = self.error * self.sigmoid_derivative(output)\n",
        "\n",
        "        self.hidden_error = self.my_dot(d_output, self.weights_hidden_output.T)\n",
        "        d_hidden = self.hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        self.weights_hidden_output += self.my_dot(self.hidden_output.T, d_output) * learning_rate\n",
        "        self.bias_hidden_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += self.my_dot(x.T, d_hidden) * learning_rate\n",
        "        self.bias_input_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, x, y, epochs, learning_rate):\n",
        "        error = 0\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(x)\n",
        "            self.backward(x, y, output, learning_rate)\n",
        "            if epoch % 100 == 0:\n",
        "                error = np.mean(np.square(y - output))\n",
        "                print(f'Epoch {epoch}: Loss = {error:.4f}')\n",
        "\n",
        "# Define XOR dataset\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Initialize and train the neural network\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(X, y, epochs=1000, learning_rate=0.5)\n",
        "\n",
        "# Test the trained model\n",
        "print(\"\\nTest the trained model:\")\n",
        "for i in range(len(X)):\n",
        "    output = nn.forward(X[i])\n",
        "    print(f\"Input: {X[i]}, Predicted Output: {output}, Actual Output: {y[i]}\")\n"
      ],
      "metadata": {
        "id": "t0syzgw16DGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fe3ef9-284f-49cd-ac6a-ba5377d0b5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.2745\n",
            "Epoch 100: Loss = 0.2452\n",
            "Epoch 200: Loss = 0.2323\n",
            "Epoch 300: Loss = 0.2060\n",
            "Epoch 400: Loss = 0.1717\n",
            "Epoch 500: Loss = 0.1299\n",
            "Epoch 600: Loss = 0.0822\n",
            "Epoch 700: Loss = 0.0473\n",
            "Epoch 800: Loss = 0.0282\n",
            "Epoch 900: Loss = 0.0182\n",
            "\n",
            "Test the trained model:\n",
            "Input: [0 0], Predicted Output: [[0.10479382]], Actual Output: [0]\n",
            "Input: [0 1], Predicted Output: [[0.87874124]], Actual Output: [1]\n",
            "Input: [1 0], Predicted Output: [[0.90248499]], Actual Output: [1]\n",
            "Input: [1 1], Predicted Output: [[0.12589268]], Actual Output: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define an abstraction in which basic computations are: additions, subtractions, multiplications, divisions or computing an activation fuction such as the sigmoid or its derivative. Then, analyze the code in detail to answer the following questions:\n",
        "\n",
        "*   How many scalar basic computations are requiered for one forward pass, for one training iteration and for a complete training process?\n",
        "*   Which are the newtwork parameters that determine the amount of computations required?\n",
        "\n",
        "**Write a formula** that gives the amount of basic scalar computations depending on the network parameters."
      ],
      "metadata": {
        "id": "BZcuGgrhqnWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Scaling ANNs\n",
        "\n",
        "In manys cases, but not all, increasing the number of layers and the number of neurons per layer leads to a higher accuracy of the model. This comes at the expense of more resources needed to run the network: memory and computation. And ultimately, it can lead to a higher application latency and energy consumption.\n",
        "\n",
        "Here you should create a fully-connected neural network based on the previous model, this time to classify handwritten numbers using the **MNIST dataset**. Investigate how to obtain the dataset and how to prepare a proper partition between training and test.\n",
        "\n",
        "The number of input neurons must be equal to the number of pixels on each image (depending on the chosen resolution). The number of output neurons must be 10, since there are 10 diffirent digits we want to classify. A new method must be included to select which of the digits was identified (by finding the most active output neuron). **Configure and test at least five versions of the model** by varying the amount of neurons in the hidden layer.\n",
        "\n",
        "Make a table or a plot to report the following for each model:\n",
        "\n",
        "*   Number of model parameters\n",
        "*   Number of basic scalar computations for a forward pass (using the previously created formula)\n",
        "*   Execution time for training and for a forward pass\n",
        "*   Model's Top-1 accuracy."
      ],
      "metadata": {
        "id": "fnwj-1yciCXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My Drive/Classroom/Embedded Machine Learning 2025-1/Labs/Solved labs/\n",
        "%ls\n",
        "\n",
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "def load_mnist_images(filename):\n",
        "\n",
        "    with open(filename, 'rb') as f:\n",
        "        data = np.fromfile(f, dtype=np.uint8, offset=16)\n",
        "    data = data.reshape(-1, 28 * 28) / 255.0\n",
        "    return data\n",
        "\n",
        "def load_mnist_labels(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        data = np.fromfile(f, dtype=np.uint8, offset=8)\n",
        "    return data\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.bias_input_hidden = np.zeros((1, hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "        self.bias_hidden_output = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward propagation through the network\n",
        "        self.hidden_output = self.sigmoid(np.dot(x, self.weights_input_hidden) + self.bias_input_hidden)\n",
        "        self.output = self.sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        # Backpropagation and weight updates\n",
        "        error = y - self.output\n",
        "        d_output = error * self.sigmoid_derivative(self.output)\n",
        "\n",
        "        hidden_error = d_output.dot(self.weights_hidden_output.T)\n",
        "        d_hidden = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * self.learning_rate\n",
        "        self.bias_hidden_output += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "        self.weights_input_hidden += x.T.dot(d_hidden) * self.learning_rate\n",
        "        self.bias_input_hidden += np.sum(d_hidden, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(X)):\n",
        "                self.forward(X[i])\n",
        "                self.backward(X[i], y[i])\n",
        "            if epoch % 10 == 0:\n",
        "                loss = np.mean(np.square(y - self.output))\n",
        "                print(f'Epoch {epoch}: Loss = {loss:.4f}')\n",
        "\n",
        "# Load MNIST data\n",
        "X_train = load_mnist_images('train-images.idx3-ubyte')\n",
        "y_train = load_mnist_labels('train-labels.idx1-ubyte')\n",
        "X_test = load_mnist_images('t10k-images.idx3-ubyte')\n",
        "y_test = load_mnist_labels('t10k-labels.idx1-ubyte')\n",
        "\n",
        "# Preprocess data\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test = np.eye(10)[y_test]\n",
        "\n",
        "# Initialize and train the neural network\n",
        "input_size = 784  # 28x28 pixels\n",
        "hidden_size = 100\n",
        "output_size = 10  # 10 classes (digits 0-9)\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate=0.1)\n",
        "nn.train(X_train, y_train, epochs=100)\n",
        "\n",
        "# Test the trained model\n",
        "correct = 0\n",
        "for i in range(len(X_test)):\n",
        "    output = nn.forward(X_test[i:i+1])\n",
        "    prediction = np.argmax(output)\n",
        "    if prediction == np.argmax(y_test[i]):\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(X_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "9FhqUJSBNRjV",
        "outputId": "8a91fa8e-9fd6-47dc-cc0c-27e0b606d538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Classroom/Embedded Machine Learning 2025-1/Labs/Solved labs\n",
            " 3-5-13-PretrainedModel.ipynb\n",
            " 4-2-12-OV7675ImageViewer.ipynb\n",
            "'Embedded ML - Lab 1.1 Native implementation of Artificial Neural Networks SOLUTION.ipynb'\n",
            "'Embedded ML - Lab 1.1_ Neural Networks in Python SOLUTION.ipynb'\n",
            "'Embedded ML - Lab 1.2_ Model Compression SOLUTION.ipynb'\n",
            "'Embedded ML - Lab 2.1_ TensorFlow SOLUTION.ipynb'\n",
            "'Embedded ML - Lab 2.2_ TensorFlow Lite SOLUTION.ipynb'\n",
            "'Embedded ML - Lab 2.3_ TensorFlow Lite Micro SOLUTION.ipynb'\n",
            "'Embedded ML - Lab 3_ ML on Embedded GPUs SOLUTION'\n",
            "'Embedded ML - Lab 4_ ML App Design.docx'\n",
            " t10k-images.idx3-ubyte\n",
            " t10k-labels.idx1-ubyte\n",
            " train-labels.idx1-ubyte\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train-images.idx3-ubyte'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-47d142a9d11b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Load MNIST data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-images.idx3-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-labels.idx1-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't10k-images.idx3-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-47d142a9d11b>\u001b[0m in \u001b[0;36mload_mnist_images\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnist_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-images.idx3-ubyte'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xr3jnLRMNw_S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}